{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoTokenizer\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.model import predict, align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/anaconda3/envs/ml/lib/python3.9/site-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model_name = \"khanhld/wav2vec2-base-vietnamese-160h\"\n",
    "model_name = \"nguyenvulebinh/wav2vec2-base-vietnamese-250h\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = f'data/train/'\n",
    "TEST_DIR = f'data/public_test'\n",
    "VERSION = '250h_pretrained'\n",
    "SAMPLING_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'valid_predictions/{VERSION}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca76eb2a5184b96be773aa20ef4fce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1057 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iters = os.listdir(f'{TRAIN_DIR}/labels/')\n",
    "for i, fname in tqdm(enumerate(iters), total=len(iters)):\n",
    "    name = fname[:-5]\n",
    "    wav, _ = librosa.load(f'{TRAIN_DIR}/songs/{name}.wav',\n",
    "                            sr = SAMPLING_RATE\n",
    "    )\n",
    "    with open(f'{TRAIN_DIR}/labels/{name}.json', 'r') as f:\n",
    "        label = json.load(f)\n",
    "\n",
    "    lyric = []\n",
    "\n",
    "    for ele in label:\n",
    "        lyric += ([w['d'] for w in ele['l']])\n",
    "        \n",
    "    lookup = predict(processor, model, wav)\n",
    "    struct = align(lyric, lookup)\n",
    "\n",
    "    with open(f'valid_predictions/{VERSION}/{name}.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(struct, f, ensure_ascii=False)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31d7989649452b8ff5b252a3e34caf45e4ffd8a5787fe28fc2ce0245f11b7782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
